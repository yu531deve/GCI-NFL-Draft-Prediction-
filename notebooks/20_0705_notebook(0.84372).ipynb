{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa576b2",
   "metadata": {},
   "source": [
    "## LightGBM用の調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "914e0c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\81807\\AppData\\Local\\Temp\\ipykernel_3628\\2063891030.py:80: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.6        0.2739726  0.65632458 0.7109375  0.62895928 0.62895928\n",
      " 0.65632458 0.62895928 0.7109375  0.71884984 0.6        0.65632458\n",
      " 0.65632458 0.71884984 0.62895928 0.62895928 0.65632458 0.71884984\n",
      " 0.62895928 0.65632458 0.65632458 0.62895928 0.62895928 0.65632458\n",
      " 0.6        0.62895928 0.62895928 0.65632458 0.71884984 0.7109375\n",
      " 0.62895928 0.7109375  0.71884984 0.62895928 0.65632458 0.62895928\n",
      " 0.2739726  0.65632458 0.65632458 0.2739726  0.68376068 0.65632458\n",
      " 0.62895928 0.48275862 0.68376068 0.68376068 0.71884984 0.62895928\n",
      " 0.62895928 0.62895928 0.7109375  0.62895928 0.71884984 0.65632458\n",
      " 0.7109375  0.62895928 0.6        0.62895928 0.7109375  0.65632458\n",
      " 0.62895928 0.62895928 0.71884984 0.71884984 0.68376068 0.7109375\n",
      " 0.68376068 0.65632458 0.62895928 0.62895928 0.62895928 0.62895928\n",
      " 0.68376068 0.6        0.71884984 0.65632458 0.6        0.71884984\n",
      " 0.68376068 0.71884984 0.68376068 0.68376068 0.2739726  0.71884984\n",
      " 0.71884984 0.65632458 0.62895928 0.68376068 0.65632458 0.68376068\n",
      " 0.62895928 0.71884984 0.65632458 0.68376068 0.62895928 0.68376068\n",
      " 0.65632458 0.6        0.62895928 0.7109375  0.68376068 0.71884984\n",
      " 0.2739726  0.65632458 0.6        0.65632458 0.71884984 0.62895928\n",
      " 0.62895928 0.62895928 0.6        0.68376068 0.68376068 0.71884984\n",
      " 0.62895928 0.71884984 0.62895928 0.68376068 0.65632458 0.65632458\n",
      " 0.65632458 0.48275862 0.68376068 0.68376068 0.6        0.71884984\n",
      " 0.65632458 0.62895928 0.62895928 0.62895928 0.71884984 0.68376068\n",
      " 0.62895928 0.71884984 0.65632458 0.71884984 0.71884984 0.62895928\n",
      " 0.65632458 0.7109375  0.7109375  0.6        0.68376068 0.71884984\n",
      " 0.6        0.68376068 0.65632458 0.71884984 0.68376068 0.6\n",
      " 0.65632458 0.7109375  0.62895928 0.68376068 0.6        0.65632458\n",
      " 0.68376068 0.2739726  0.65632458 0.6        0.65632458 0.71884984\n",
      " 0.7109375  0.65632458 0.65632458 0.62895928 0.65632458 0.62895928\n",
      " 0.71884984 0.6        0.6        0.68376068 0.62895928 0.62895928\n",
      " 0.65632458 0.62895928 0.7109375  0.65632458 0.7109375  0.62895928\n",
      " 0.65632458 0.62895928 0.71884984 0.6        0.65632458 0.65632458\n",
      " 0.71884984 0.65632458 0.2739726  0.7109375  0.65632458 0.68376068\n",
      " 0.62895928 0.6        0.48275862 0.2739726  0.7109375  0.71884984\n",
      " 0.65632458 0.62895928 0.65632458 0.65632458 0.71884984 0.6\n",
      " 0.68376068 0.62895928 0.62895928 0.71884984 0.65632458 0.62895928\n",
      " 0.68376068 0.71884984 0.65632458 0.2739726  0.62895928 0.68376068\n",
      " 0.62895928 0.7109375  0.62895928 0.68376068 0.62895928 0.7109375\n",
      " 0.7109375  0.68376068 0.71884984 0.7109375  0.48275862 0.65632458\n",
      " 0.65632458 0.6        0.68376068 0.7109375  0.68376068 0.62895928\n",
      " 0.68376068 0.6        0.65632458 0.71884984 0.71884984 0.71884984\n",
      " 0.62895928 0.7109375  0.7109375  0.65632458 0.62895928 0.65632458\n",
      " 0.71884984 0.7109375  0.62895928 0.68376068 0.65632458 0.68376068\n",
      " 0.2739726  0.62895928 0.65632458 0.62895928 0.6        0.71884984\n",
      " 0.7109375  0.62895928 0.65632458 0.68376068 0.71884984 0.62895928\n",
      " 0.62895928 0.65632458 0.62895928 0.62895928 0.71884984 0.65632458\n",
      " 0.68376068 0.62895928 0.62895928 0.71884984 0.68376068 0.68376068\n",
      " 0.6        0.6        0.62895928 0.62895928 0.71884984 0.48275862\n",
      " 0.62895928 0.65632458 0.62895928 0.65632458 0.65632458 0.62895928\n",
      " 0.62895928 0.62895928 0.7109375  0.62895928 0.6        0.7109375\n",
      " 0.7109375  0.62895928 0.62895928 0.62895928 0.68376068 0.7109375\n",
      " 0.68376068 0.7109375  0.62895928 0.68376068 0.68376068 0.62895928\n",
      " 0.2739726  0.65632458 0.71884984 0.71884984 0.65632458 0.65632458\n",
      " 0.6        0.2739726  0.68376068 0.65632458 0.68376068 0.7109375\n",
      " 0.6        0.65632458 0.2739726  0.68376068 0.62895928 0.68376068\n",
      " 0.62895928 0.2739726  0.65632458 0.68376068 0.48275862 0.65632458\n",
      " 0.68376068 0.62895928 0.65632458 0.7109375  0.7109375  0.71884984\n",
      " 0.6        0.6        0.65632458 0.71884984 0.62895928 0.71884984\n",
      " 0.65632458 0.48275862 0.71884984 0.71884984 0.7109375  0.65632458\n",
      " 0.71884984 0.62895928 0.65632458 0.6        0.62895928 0.7109375\n",
      " 0.7109375  0.62895928 0.65632458 0.7109375  0.65632458 0.71884984\n",
      " 0.68376068 0.65632458 0.65632458 0.65632458 0.62895928 0.62895928\n",
      " 0.7109375  0.62895928 0.6        0.6        0.2739726  0.2739726\n",
      " 0.65632458 0.71884984 0.65632458 0.68376068 0.62895928 0.65632458\n",
      " 0.65632458 0.62895928 0.71884984 0.65632458 0.65632458 0.68376068\n",
      " 0.68376068 0.68376068 0.65632458 0.68376068 0.62895928 0.65632458\n",
      " 0.62895928 0.68376068 0.65632458 0.65632458 0.62895928 0.48275862\n",
      " 0.68376068 0.68376068 0.71884984 0.62895928 0.62895928 0.65632458\n",
      " 0.71884984 0.68376068 0.68376068 0.65632458 0.62895928 0.65632458\n",
      " 0.65632458 0.68376068 0.65632458 0.62895928 0.68376068 0.71884984\n",
      " 0.2739726  0.71884984 0.71884984 0.68376068 0.65632458 0.65632458\n",
      " 0.62895928 0.62895928 0.62895928 0.71884984 0.62895928 0.68376068\n",
      " 0.48275862 0.65632458 0.62895928 0.65632458 0.65632458 0.71884984\n",
      " 0.68376068 0.62895928 0.68376068 0.2739726  0.65632458 0.68376068\n",
      " 0.62895928 0.2739726  0.48275862 0.71884984 0.65632458 0.62895928\n",
      " 0.68376068 0.2739726  0.68376068 0.71884984 0.2739726  0.65632458\n",
      " 0.65632458 0.65632458 0.62895928 0.65632458 0.62895928 0.7109375\n",
      " 0.65632458 0.71884984 0.71884984 0.7109375  0.62895928 0.62895928\n",
      " 0.62895928 0.62895928 0.7109375  0.62895928 0.65632458 0.62895928\n",
      " 0.62895928 0.65632458 0.68376068 0.71884984 0.6        0.68376068\n",
      " 0.65632458 0.65632458 0.62895928 0.71884984 0.65632458 0.65632458\n",
      " 0.71884984 0.71884984 0.62895928 0.62895928 0.6        0.62895928\n",
      " 0.65632458 0.71884984 0.65632458 0.71884984 0.6        0.62895928\n",
      " 0.71884984 0.62895928 0.62895928 0.48275862 0.68376068 0.62895928\n",
      " 0.7109375  0.7109375  0.6        0.62895928 0.62895928 0.6\n",
      " 0.65632458 0.65632458 0.62895928 0.71884984 0.6        0.7109375\n",
      " 0.68376068 0.7109375  0.71884984 0.6        0.6        0.62895928\n",
      " 0.7109375  0.68376068 0.2739726  0.62895928 0.48275862 0.7109375\n",
      " 0.62895928 0.68376068 0.62895928 0.7109375  0.62895928 0.62895928\n",
      " 0.62895928 0.71884984 0.71884984 0.62895928 0.65632458 0.65632458\n",
      " 0.62895928 0.7109375  0.68376068 0.65632458 0.68376068 0.68376068\n",
      " 0.65632458 0.62895928 0.7109375  0.7109375  0.62895928 0.68376068\n",
      " 0.7109375  0.62895928 0.65632458 0.65632458 0.68376068 0.6\n",
      " 0.68376068 0.62895928 0.62895928 0.68376068 0.71884984]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[val_idx, \"Position_group_encoded\"] = X.loc[val_idx, \"Position_group\"].map(group_map)\n"
     ]
    }
   ],
   "source": [
    "# ✅ 必要ライブラリ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ✅ データ読み込み\n",
    "PATH = '../data/'\n",
    "X = pd.read_csv(PATH + 'train.csv')\n",
    "test = pd.read_csv(PATH + 'test.csv')\n",
    "\n",
    "# ✅ Age 欠損処理\n",
    "X[\"Age_filled\"] = X[\"Age\"].fillna(-1)\n",
    "X[\"Age_missing\"] = X[\"Age\"].isna().astype(int)\n",
    "X = X.drop(columns=[\"Age\"])\n",
    "test[\"Age_filled\"] = test[\"Age\"].fillna(-1)\n",
    "test[\"Age_missing\"] = test[\"Age\"].isna().astype(int)\n",
    "test = test.drop(columns=[\"Age\"])\n",
    "\n",
    "# ✅ 数値カラム欠損補完\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        median = X[col].median()\n",
    "        X[col] = X[col].fillna(median)\n",
    "        test[col] = test[col].fillna(median)\n",
    "\n",
    "# ✅ 不要カラム削除\n",
    "X = X.drop(columns=[\"Id\"])\n",
    "test = test.drop(columns=[\"Id\"])\n",
    "\n",
    "# ✅ 目的変数分離\n",
    "y = X[\"Drafted\"]\n",
    "X = X.drop(columns=[\"Drafted\"])\n",
    "\n",
    "# ✅ School, Player_Type, Position_Type 削除\n",
    "X = X.drop(columns=[\"School\", \"Player_Type\", \"Position_Type\"])\n",
    "test = test.drop(columns=[\"School\", \"Player_Type\", \"Position_Type\"])\n",
    "\n",
    "# ✅ Position Target Encoding\n",
    "position_stats = X.copy()\n",
    "position_stats[\"Drafted\"] = y\n",
    "position_target_map = position_stats.groupby(\"Position\")[\"Drafted\"].mean()\n",
    "X[\"Position_encoded\"] = X[\"Position\"].map(position_target_map)\n",
    "test[\"Position_encoded\"] = test[\"Position\"].map(position_target_map)\n",
    "test[\"Position_encoded\"] = test[\"Position_encoded\"].fillna(X[\"Position_encoded\"].mean())\n",
    "\n",
    "# ✅ Position Group Encoding (fold-safe)\n",
    "def map_position_group(pos):\n",
    "    if pos in [\"K\", \"P\", \"LS\"]:\n",
    "        return \"Specialist\"\n",
    "    elif pos in [\"WR\", \"RB\", \"TE\"]:\n",
    "        return \"OffensiveSkill\"\n",
    "    elif pos in [\"OT\", \"OG\", \"C\"]:\n",
    "        return \"OffensiveLine\"\n",
    "    elif pos in [\"DE\", \"DT\"]:\n",
    "        return \"DefensiveLine\"\n",
    "    elif pos in [\"OLB\", \"ILB\"]:\n",
    "        return \"Linebacker\"\n",
    "    elif pos in [\"CB\", \"FS\", \"SS\", \"S\", \"DB\"]:\n",
    "        return \"DefensiveBack\"\n",
    "    elif pos == \"QB\":\n",
    "        return \"Quarterback\"\n",
    "    elif pos == \"FB\":\n",
    "        return \"Fullback\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "X[\"Position_group\"] = X[\"Position\"].apply(map_position_group)\n",
    "test[\"Position_group\"] = test[\"Position\"].apply(map_position_group)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X[\"Position_group_encoded\"] = 0\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    group_map = X_train.assign(Drafted=y_train).groupby(\"Position_group\")[\"Drafted\"].mean()\n",
    "    X.loc[val_idx, \"Position_group_encoded\"] = X.loc[val_idx, \"Position_group\"].map(group_map)\n",
    "final_group_map = X.assign(Drafted=y).groupby(\"Position_group\")[\"Drafted\"].mean()\n",
    "test[\"Position_group_encoded\"] = test[\"Position_group\"].map(final_group_map)\n",
    "test[\"Position_group_encoded\"] = test[\"Position_group_encoded\"].fillna(X[\"Position_group_encoded\"].mean())\n",
    "\n",
    "X = X.drop(columns=[\"Position\", \"Position_group\"])\n",
    "test = test.drop(columns=[\"Position\", \"Position_group\"])\n",
    "\n",
    "# ✅ SpeedScore, BurstScore, AgilityScore, ASI, RSA特徴量\n",
    "X[\"Weight_lbs\"] = X[\"Weight\"] * 2.20462\n",
    "test[\"Weight_lbs\"] = test[\"Weight\"] * 2.20462\n",
    "\n",
    "X[\"SpeedScore\"] = X[\"Weight_lbs\"] * (200 / X[\"Sprint_40yd\"]**2)\n",
    "test[\"SpeedScore\"] = test[\"Weight_lbs\"] * (200 / test[\"Sprint_40yd\"]**2)\n",
    "\n",
    "X[\"BurstScore\"] = X[\"Vertical_Jump\"] + X[\"Broad_Jump\"]\n",
    "test[\"BurstScore\"] = test[\"Vertical_Jump\"] + test[\"Broad_Jump\"]\n",
    "\n",
    "X[\"AgilityScore\"] = X[\"Shuttle\"] + X[\"Agility_3cone\"]\n",
    "test[\"AgilityScore\"] = test[\"Shuttle\"] + test[\"Agility_3cone\"]\n",
    "\n",
    "X[\"ASI\"] = 0.5 * X[\"SpeedScore\"] + 0.3 * X[\"BurstScore\"] + 0.2 * X[\"AgilityScore\"]\n",
    "test[\"ASI\"] = 0.5 * test[\"SpeedScore\"] + 0.3 * test[\"BurstScore\"] + 0.2 * test[\"AgilityScore\"]\n",
    "\n",
    "rsa_features = [\"Sprint_40yd\", \"Vertical_Jump\", \"Bench_Press_Reps\", \"Shuttle\", \"Agility_3cone\"]\n",
    "for col in rsa_features:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "    if col in [\"Sprint_40yd\", \"Shuttle\", \"Agility_3cone\"]:\n",
    "        X[f\"RSA_{col}\"] = 10 - scaler.fit_transform(X[[col]])\n",
    "        test[f\"RSA_{col}\"] = 10 - scaler.transform(test[[col]])\n",
    "    else:\n",
    "        X[f\"RSA_{col}\"] = scaler.fit_transform(X[[col]])\n",
    "        test[f\"RSA_{col}\"] = scaler.transform(test[[col]])\n",
    "\n",
    "# ✅ BMI\n",
    "X[\"BMI\"] = X[\"Weight\"] / (X[\"Height\"]/100)**2\n",
    "test[\"BMI\"] = test[\"Weight\"] / (test[\"Height\"]/100)**2\n",
    "\n",
    "# ✅ School特徴量（Top, Drafted Count, Drafted Rate TE）\n",
    "df_raw = pd.read_csv(PATH + 'train.csv')\n",
    "test_raw = pd.read_csv(PATH + 'test.csv')\n",
    "X[\"School\"] = df_raw[\"School\"]\n",
    "test[\"School\"] = test_raw[\"School\"]\n",
    "\n",
    "school_stats = X.copy()\n",
    "school_stats[\"Drafted\"] = y\n",
    "school_agg = school_stats.groupby(\"School\")[\"Drafted\"].agg([\"sum\", \"count\"])\n",
    "school_agg[\"Drafted_Rate\"] = school_agg[\"sum\"] / school_agg[\"count\"]\n",
    "\n",
    "top_n = 20\n",
    "top_schools = school_agg[\"sum\"].sort_values(ascending=False).head(top_n).index.tolist()\n",
    "\n",
    "X[\"School_Top\"] = X[\"School\"].isin(top_schools).astype(int)\n",
    "test[\"School_Top\"] = test[\"School\"].isin(top_schools).astype(int)\n",
    "\n",
    "X[\"School_Drafted_Count\"] = X[\"School\"].map(school_agg[\"sum\"])\n",
    "test[\"School_Drafted_Count\"] = test[\"School\"].map(school_agg[\"sum\"])\n",
    "test[\"School_Drafted_Count\"] = test[\"School_Drafted_Count\"].fillna(0)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X[\"School_Drafted_Rate_TE\"] = 0.0\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    rate_map = X_train.assign(Drafted=y_train).groupby(\"School\")[\"Drafted\"].mean()\n",
    "    X.loc[val_idx, \"School_Drafted_Rate_TE\"] = X.loc[val_idx, \"School\"].map(rate_map)\n",
    "final_rate_map = X.assign(Drafted=y).groupby(\"School\")[\"Drafted\"].mean()\n",
    "test[\"School_Drafted_Rate_TE\"] = test[\"School\"].map(final_rate_map)\n",
    "test[\"School_Drafted_Rate_TE\"] = test[\"School_Drafted_Rate_TE\"].fillna(y.mean())\n",
    "\n",
    "X = X.drop(columns=[\"School\"])\n",
    "test = test.drop(columns=[\"School\"])\n",
    "\n",
    "\n",
    "X[\"Speed_BMI_Ratio\"] = X[\"SpeedScore\"] / X[\"BMI\"]\n",
    "test[\"Speed_BMI_Ratio\"] = test[\"SpeedScore\"] / test[\"BMI\"]\n",
    "\n",
    "X[\"Sprint_ASI\"] = X[\"Sprint_40yd\"] * X[\"ASI\"]\n",
    "test[\"Sprint_ASI\"] = test[\"Sprint_40yd\"] * test[\"ASI\"]\n",
    "\n",
    "X[\"Age_Speed\"] = X[\"Age_filled\"] * X[\"SpeedScore\"]\n",
    "test[\"Age_Speed\"] = test[\"Age_filled\"] * test[\"SpeedScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "587cd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 高相関特徴量削除\n",
    "drop_cols_high_corr = [\n",
    "    \"Weight_lbs\",\n",
    "    \"RSA_Sprint_40yd\",\n",
    "    \"RSA_Vertical_Jump\",\n",
    "    \"RSA_Bench_Press_Reps\",\n",
    "    \"RSA_Agility_3cone\",\n",
    "    \"RSA_Shuttle\",\n",
    "    \"SpeedScore\",\n",
    "    \"Age_missing\"\n",
    "]\n",
    "X = X.drop(columns=drop_cols_high_corr)\n",
    "test = test.drop(columns=drop_cols_high_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50bbce",
   "metadata": {},
   "source": [
    "# CatBoostのOptunaによる最適化（特徴量などは今後いじる）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6feeb9",
   "metadata": {},
   "source": [
    "```python\n",
    "import optuna\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 200, 1000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.2),\n",
    "        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-2, 10),\n",
    "        \"bagging_temperature\": trial.suggest_uniform(\"bagging_temperature\", 0, 1),\n",
    "        \"random_strength\": trial.suggest_uniform(\"random_strength\", 0, 1),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "        \"task_type\": \"CPU\",  # GPUがなければCPUに変更\n",
    "        \"verbose\": 0,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    aucs = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        train_pool = Pool(X_train, y_train)\n",
    "        valid_pool = Pool(X_valid, y_valid)\n",
    "\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=30, verbose=0)\n",
    "\n",
    "        y_valid_pred = model.predict_proba(X_valid)[:, 1]\n",
    "        auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"✅ Best params:\", study.best_params)\n",
    "print(\"✅ Best CV AUC:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f99621",
   "metadata": {},
   "source": [
    "✅ Best params: {'iterations': 993, 'depth': 5, 'learning_rate': 0.07455966036915365, 'l2_leaf_reg': 0.010557909406604652, 'bagging_temperature': 0.733088086114168, 'random_strength': 0.6639499886391435, 'border_count': 177}\n",
    "✅ Best CV AUC: 0.8511843672980401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb15ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CatBoost 最適化パラメータで再学習開始\n",
      "✅ [CatBoost] Fold 1 - Train AUC: 0.9226, Validation AUC: 0.8684\n",
      "✅ [CatBoost] Fold 2 - Train AUC: 0.9509, Validation AUC: 0.8543\n",
      "✅ [CatBoost] Fold 3 - Train AUC: 0.9324, Validation AUC: 0.8344\n",
      "✅ [CatBoost] Fold 4 - Train AUC: 0.8573, Validation AUC: 0.8132\n",
      "✅ [CatBoost] Fold 5 - Train AUC: 0.8774, Validation AUC: 0.8568\n",
      "\n",
      "✅ [CatBoost] Average Train AUC: 0.9081\n",
      "✅ [CatBoost] Average Validation AUC: 0.8454\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=993,\n",
    "    depth=5,\n",
    "    learning_rate=0.07455966036915365,\n",
    "    l2_leaf_reg=0.010557909406604652,\n",
    "    bagging_temperature=0.733088086114168,\n",
    "    random_strength=0.6639499886391435,\n",
    "    border_count=177,\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_aucs = []\n",
    "val_aucs = []\n",
    "\n",
    "print(\"✅ CatBoost 最適化パラメータで再学習開始\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=30, use_best_model=True)\n",
    "    \n",
    "    y_train_pred = cat_model.predict_proba(X_train)[:, 1]\n",
    "    y_valid_pred = cat_model.predict_proba(X_valid)[:, 1]\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    val_auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "    \n",
    "    train_aucs.append(train_auc)\n",
    "    val_aucs.append(val_auc)\n",
    "    \n",
    "    print(f\"✅ [CatBoost] Fold {fold + 1} - Train AUC: {train_auc:.4f}, Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ [CatBoost] Average Train AUC: {np.mean(train_aucs):.4f}\")\n",
    "print(f\"✅ [CatBoost] Average Validation AUC: {np.mean(val_aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a305e",
   "metadata": {},
   "source": [
    "# XGBoostのOptunaによる最適化（特徴量などは今後いじる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a332106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e152ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-auc:0.80475\n",
      "[100]\tvalid-auc:0.86342\n",
      "[132]\tvalid-auc:0.86196\n",
      "✅ [XGBoost] Fold 1 - Train AUC: 0.9619, Validation AUC: 0.8612\n",
      "[0]\tvalid-auc:0.77742\n",
      "[100]\tvalid-auc:0.85508\n",
      "[142]\tvalid-auc:0.85547\n",
      "✅ [XGBoost] Fold 2 - Train AUC: 0.9682, Validation AUC: 0.8555\n",
      "[0]\tvalid-auc:0.76348\n",
      "[35]\tvalid-auc:0.81223\n",
      "✅ [XGBoost] Fold 3 - Train AUC: 0.9076, Validation AUC: 0.8122\n",
      "[0]\tvalid-auc:0.79296\n",
      "[30]\tvalid-auc:0.80043\n",
      "✅ [XGBoost] Fold 4 - Train AUC: 0.8996, Validation AUC: 0.8014\n",
      "[0]\tvalid-auc:0.81862\n",
      "[100]\tvalid-auc:0.86026\n",
      "[107]\tvalid-auc:0.86029\n",
      "✅ [XGBoost] Fold 5 - Train AUC: 0.9500, Validation AUC: 0.8603\n",
      "\n",
      "✅ [XGBoost] Average Train AUC: 0.9375\n",
      "✅ [XGBoost] Average Validation AUC: 0.8381\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1.0,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_aucs = []\n",
    "val_aucs = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "    \n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dvalid, 'valid')],\n",
    "        early_stopping_rounds=30,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    y_train_pred = model.predict(dtrain)\n",
    "    y_valid_pred = model.predict(dvalid)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    val_auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "    \n",
    "    train_aucs.append(train_auc)\n",
    "    val_aucs.append(val_auc)\n",
    "    \n",
    "    print(f\"✅ [XGBoost] Fold {fold+1} - Train AUC: {train_auc:.4f}, Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ [XGBoost] Average Train AUC: {np.mean(train_aucs):.4f}\")\n",
    "print(f\"✅ [XGBoost] Average Validation AUC: {np.mean(val_aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf269a",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Optuna objective\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": 1000,\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_uniform(\"gamma\", 0, 5),\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-3, 10.0),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-3, 10.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"random_state\": 42,\n",
    "        \"tree_method\": \"hist\",  # CPUなら \"hist\"\n",
    "        \"use_label_encoder\": False,\n",
    "        \"eval_metric\": \"auc\"\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    aucs = []\n",
    "\n",
    "    for train_idx, valid_idx in cv.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        model = XGBClassifier(**params)\n",
    "\n",
    "        # ✅ カラム数エラー防止のため eval_set 使用時は verbose=0\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        y_valid_pred = model.predict_proba(X_valid)[:, 1]\n",
    "        auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return np.mean(aucs)\n",
    "\n",
    "# ✅ Optuna 実行\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(f\"✅ Best params: {study.best_params}\")\n",
    "print(f\"✅ Best CV AUC: {study.best_value:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e6c64",
   "metadata": {},
   "source": [
    "✅ Best params: {'max_depth': 3, 'learning_rate': 0.01813146526150014, 'subsample': 0.8306991416735662, 'colsample_bytree': 0.9931326288237515, 'gamma': 0.6283945388318324, 'reg_alpha': 0.08282377808150526, 'reg_lambda': 0.1838172412783487, 'min_child_weight': 1}\n",
    "✅ Best CV AUC: 0.850278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be2f4c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost 最適化パラメータで再学習開始\n",
      "[0]\tvalid-auc:0.81306\n",
      "[100]\tvalid-auc:0.84525\n",
      "[200]\tvalid-auc:0.85822\n",
      "[300]\tvalid-auc:0.86526\n",
      "[400]\tvalid-auc:0.86784\n",
      "✅ [XGBoost] Fold 1 - Train AUC: 0.9146, Validation AUC: 0.8681\n",
      "[0]\tvalid-auc:0.77620\n",
      "[100]\tvalid-auc:0.84471\n",
      "[200]\tvalid-auc:0.85515\n",
      "[300]\tvalid-auc:0.86162\n",
      "[400]\tvalid-auc:0.86351\n",
      "[409]\tvalid-auc:0.86347\n",
      "✅ [XGBoost] Fold 2 - Train AUC: 0.9192, Validation AUC: 0.8635\n",
      "[0]\tvalid-auc:0.76874\n",
      "[100]\tvalid-auc:0.82217\n",
      "[200]\tvalid-auc:0.82742\n",
      "[300]\tvalid-auc:0.83053\n",
      "[322]\tvalid-auc:0.82979\n",
      "✅ [XGBoost] Fold 3 - Train AUC: 0.9104, Validation AUC: 0.8298\n",
      "[0]\tvalid-auc:0.75801\n",
      "[39]\tvalid-auc:0.79494\n",
      "✅ [XGBoost] Fold 4 - Train AUC: 0.8581, Validation AUC: 0.7949\n",
      "[0]\tvalid-auc:0.77890\n",
      "[100]\tvalid-auc:0.85050\n",
      "[200]\tvalid-auc:0.86061\n",
      "[300]\tvalid-auc:0.86528\n",
      "[387]\tvalid-auc:0.86603\n",
      "✅ [XGBoost] Fold 5 - Train AUC: 0.9136, Validation AUC: 0.8659\n",
      "\n",
      "✅ [XGBoost] Average Train AUC: 0.9032\n",
      "✅ [XGBoost] Average Validation AUC: 0.8444\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.01813146526150014,\n",
    "    'subsample': 0.8306991416735662,\n",
    "    'colsample_bytree': 0.9931326288237515,\n",
    "    'gamma': 0.6283945388318324,\n",
    "    'reg_alpha': 0.08282377808150526,\n",
    "    'reg_lambda': 0.1838172412783487,\n",
    "    'min_child_weight': 1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_aucs = []\n",
    "val_aucs = []\n",
    "\n",
    "print(\"✅ XGBoost 最適化パラメータで再学習開始\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "    \n",
    "    model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dvalid, 'valid')],\n",
    "        early_stopping_rounds=30,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    y_train_pred = model.predict(dtrain)\n",
    "    y_valid_pred = model.predict(dvalid)\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    val_auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "    \n",
    "    train_aucs.append(train_auc)\n",
    "    val_aucs.append(val_auc)\n",
    "    \n",
    "    print(f\"✅ [XGBoost] Fold {fold + 1} - Train AUC: {train_auc:.4f}, Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ [XGBoost] Average Train AUC: {np.mean(train_aucs):.4f}\")\n",
    "print(f\"✅ [XGBoost] Average Validation AUC: {np.mean(val_aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146d5fa",
   "metadata": {},
   "source": [
    "## ✅ LightGBM 最適化パラメータ（直近ベストスコア使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d065a553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LightGBM 最適化パラメータで再学習開始\n",
      "[LightGBM] [Info] Number of positive: 1445, number of negative: 779\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2589\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649730 -> initscore=0.617854\n",
      "[LightGBM] [Info] Start training from score 0.617854\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's auc: 0.857416\tvalid_0's binary_logloss: 0.418876\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid_0's auc: 0.858342\tvalid_0's binary_logloss: 0.416497\n",
      "✅ [LightGBM] Fold 1 - Train AUC: 0.8853, Validation AUC: 0.8583\n",
      "[LightGBM] [Info] Number of positive: 1448, number of negative: 777\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2594\n",
      "[LightGBM] [Info] Number of data points in the train set: 2225, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650787 -> initscore=0.622498\n",
      "[LightGBM] [Info] Start training from score 0.622498\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's auc: 0.844692\tvalid_0's binary_logloss: 0.433368\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's auc: 0.845183\tvalid_0's binary_logloss: 0.438607\n",
      "✅ [LightGBM] Fold 2 - Train AUC: 0.8830, Validation AUC: 0.8452\n",
      "[LightGBM] [Info] Number of positive: 1442, number of negative: 783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2603\n",
      "[LightGBM] [Info] Number of data points in the train set: 2225, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.648090 -> initscore=0.610654\n",
      "[LightGBM] [Info] Start training from score 0.610654\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's auc: 0.816692\tvalid_0's binary_logloss: 0.462228\n",
      "✅ [LightGBM] Fold 3 - Train AUC: 0.8824, Validation AUC: 0.8167\n",
      "[LightGBM] [Info] Number of positive: 1438, number of negative: 787\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2607\n",
      "[LightGBM] [Info] Number of data points in the train set: 2225, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.646292 -> initscore=0.602780\n",
      "[LightGBM] [Info] Start training from score 0.602780\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's auc: 0.806971\tvalid_0's binary_logloss: 0.450961\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's auc: 0.807359\tvalid_0's binary_logloss: 0.451815\n",
      "✅ [LightGBM] Fold 4 - Train AUC: 0.8867, Validation AUC: 0.8074\n",
      "[LightGBM] [Info] Number of positive: 1439, number of negative: 786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2600\n",
      "[LightGBM] [Info] Number of data points in the train set: 2225, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.646742 -> initscore=0.604747\n",
      "[LightGBM] [Info] Start training from score 0.604747\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's auc: 0.855676\tvalid_0's binary_logloss: 0.419737\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's auc: 0.856907\tvalid_0's binary_logloss: 0.420701\n",
      "✅ [LightGBM] Fold 5 - Train AUC: 0.8831, Validation AUC: 0.8569\n",
      "\n",
      "✅ [LightGBM] Average Train AUC: 0.8841\n",
      "✅ [LightGBM] Average Validation AUC: 0.8369\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# ✅ LightGBM 最適化パラメータ（直近ベストスコア使用）\n",
    "model_lgb = LGBMClassifier(\n",
    "    max_depth=6,\n",
    "    num_leaves=10,\n",
    "    min_child_samples=38,\n",
    "    reg_alpha=8.18,\n",
    "    reg_lambda=8.07,\n",
    "    learning_rate=0.0442,\n",
    "    n_estimators=1000,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_aucs_lgb = []\n",
    "val_aucs_lgb = []\n",
    "\n",
    "print(\"✅ LightGBM 最適化パラメータで再学習開始\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    model_lgb.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=30), lgb.log_evaluation(100)]\n",
    "    )\n",
    "\n",
    "    y_train_pred = model_lgb.predict_proba(X_train)[:, 1]\n",
    "    y_valid_pred = model_lgb.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    val_auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "\n",
    "    train_aucs_lgb.append(train_auc)\n",
    "    val_aucs_lgb.append(val_auc)\n",
    "\n",
    "    print(f\"✅ [LightGBM] Fold {fold + 1} - Train AUC: {train_auc:.4f}, Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ [LightGBM] Average Train AUC: {np.mean(train_aucs_lgb):.4f}\")\n",
    "print(f\"✅ [LightGBM] Average Validation AUC: {np.mean(val_aucs_lgb):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ccbdfa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ アンサンブル (Weighted Voting + Stacking) 開始\n",
      "[0]\tvalid-auc:0.81306\n",
      "[100]\tvalid-auc:0.84525\n",
      "[200]\tvalid-auc:0.85823\n",
      "[300]\tvalid-auc:0.86526\n",
      "[400]\tvalid-auc:0.86783\n",
      "[401]\tvalid-auc:0.86808\n",
      "[LightGBM] [Info] Number of positive: 1445, number of negative: 779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2589\n",
      "[LightGBM] [Info] Number of data points in the train set: 2224, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649730 -> initscore=0.617854\n",
      "[LightGBM] [Info] Start training from score 0.617854\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's auc: 0.857416\tvalid_0's binary_logloss: 0.418876\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid_0's auc: 0.858342\tvalid_0's binary_logloss: 0.416497\n",
      "[0]\tvalid-auc:0.77620\n",
      "[100]\tvalid-auc:0.84471\n",
      "[200]\tvalid-auc:0.85516\n",
      "[300]\tvalid-auc:0.86162\n",
      "[400]\tvalid-auc:0.86351\n",
      "[409]\tvalid-auc:0.86347\n",
      "[LightGBM] [Info] Number of positive: 1448, number of negative: 777\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2594\n",
      "[LightGBM] [Info] Number of data points in the train set: 2225, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650787 -> initscore=0.622498\n",
      "[LightGBM] [Info] Start training from score 0.622498\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's auc: 0.844692\tvalid_0's binary_logloss: 0.433368\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's auc: 0.845183\tvalid_0's binary_logloss: 0.438607\n",
      "[0]\tvalid-auc:0.76874\n",
      "[100]\tvalid-auc:0.82217\n",
      "[200]\tvalid-auc:0.82732\n",
      "[295]\tvalid-auc:0.83027\n",
      "[LightGBM] [Info] Number of positive: 1442, number of negative: 783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2603\n",
      "[LightGBM] [Info] Number of data points in the train set: 2225, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.648090 -> initscore=0.610654\n",
      "[LightGBM] [Info] Start training from score 0.610654\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's auc: 0.816692\tvalid_0's binary_logloss: 0.462228\n",
      "[0]\tvalid-auc:0.75801\n",
      "[38]\tvalid-auc:0.79532\n",
      "[LightGBM] [Info] Number of positive: 1438, number of negative: 787\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2607\n",
      "[LightGBM] [Info] Number of data points in the train set: 2225, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.646292 -> initscore=0.602780\n",
      "[LightGBM] [Info] Start training from score 0.602780\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tvalid_0's auc: 0.806971\tvalid_0's binary_logloss: 0.450961\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid_0's auc: 0.807359\tvalid_0's binary_logloss: 0.451815\n",
      "[0]\tvalid-auc:0.77890\n",
      "[100]\tvalid-auc:0.85050\n",
      "[200]\tvalid-auc:0.86059\n",
      "[300]\tvalid-auc:0.86528\n",
      "[387]\tvalid-auc:0.86601\n",
      "[LightGBM] [Info] Number of positive: 1439, number of negative: 786\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2600\n",
      "[LightGBM] [Info] Number of data points in the train set: 2225, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.646742 -> initscore=0.604747\n",
      "[LightGBM] [Info] Start training from score 0.604747\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\tvalid_0's auc: 0.855676\tvalid_0's binary_logloss: 0.419737\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's auc: 0.856907\tvalid_0's binary_logloss: 0.420701\n",
      "\n",
      "✅ [Weighted Voting] Validation AUC: 0.8440\n",
      "✅ [Stacking] Validation AUC: 0.8452\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "assert isinstance(y, pd.Series)  # 安全確認\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_cat = np.zeros(len(X))\n",
    "oof_xgb = np.zeros(len(X))\n",
    "oof_lgb = np.zeros(len(X))\n",
    "y_true = np.zeros(len(X))\n",
    "\n",
    "# ✅ CatBoost 最適化済パラメータ\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=993,\n",
    "    depth=5,\n",
    "    learning_rate=0.07456,\n",
    "    l2_leaf_reg=0.01056,\n",
    "    bagging_temperature=0.7331,\n",
    "    random_strength=0.6640,\n",
    "    border_count=177,\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# ✅ XGBoost 最適化済パラメータ\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.01813,\n",
    "    'subsample': 0.8307,\n",
    "    'colsample_bytree': 0.9931,\n",
    "    'gamma': 0.6284,\n",
    "    'reg_alpha': 0.0828,\n",
    "    'reg_lambda': 0.1838,\n",
    "    'min_child_weight': 1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# ✅ LightGBM 最適化済パラメータ\n",
    "lgb_model = LGBMClassifier(\n",
    "    max_depth=6,\n",
    "    num_leaves=10,\n",
    "    min_child_samples=38,\n",
    "    reg_alpha=8.18,\n",
    "    reg_lambda=8.07,\n",
    "    learning_rate=0.0442,\n",
    "    n_estimators=1000,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ アンサンブル (Weighted Voting + Stacking) 開始\")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    # CatBoost\n",
    "    cat_model.fit(X_train, y_train, eval_set=(X_valid, y_valid),\n",
    "                  early_stopping_rounds=30, use_best_model=True)\n",
    "    oof_cat[valid_idx] = cat_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    # XGBoost\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "    xgb_model = xgb.train(xgb_params, dtrain, num_boost_round=2000,\n",
    "                           evals=[(dvalid, 'valid')], early_stopping_rounds=30, verbose_eval=100)\n",
    "    oof_xgb[valid_idx] = xgb_model.predict(dvalid)\n",
    "\n",
    "    # LightGBM\n",
    "    lgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=30), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    oof_lgb[valid_idx] = lgb_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    y_true[valid_idx] = y_valid\n",
    "\n",
    "# ✅ Weighted Soft Voting\n",
    "weights = [1, 1, 1]  # 必要に応じて調整\n",
    "ensemble_probs = (weights[0] * oof_cat + weights[1] * oof_xgb + weights[2] * oof_lgb) / sum(weights)\n",
    "auc_voting = roc_auc_score(y_true, ensemble_probs)\n",
    "print(f\"\\n✅ [Weighted Voting] Validation AUC: {auc_voting:.4f}\")\n",
    "\n",
    "# ✅ Stacking\n",
    "stack_X = np.vstack([oof_cat, oof_xgb, oof_lgb]).T\n",
    "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "meta_model.fit(stack_X, y_true)\n",
    "meta_preds = meta_model.predict_proba(stack_X)[:, 1]\n",
    "auc_stacking = roc_auc_score(y_true, meta_preds)\n",
    "print(f\"✅ [Stacking] Validation AUC: {auc_stacking:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4752558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ アンサンブル提出ファイルを保存しました: c:\\Users\\81807\\Desktop\\Kaggle\\GCI②(NFL Draft Prediction)\\submissions\\submission_20_0705().csv\n"
     ]
    }
   ],
   "source": [
    "# ✅ 提出用ファイル作成処理（アンサンブル予測）\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# ✅ テストデータ再読み込み（Id復元用）\n",
    "original_test = pd.read_csv(PATH + \"test.csv\")\n",
    "\n",
    "# ✅ 提出用特徴量列（現在のX.columnsで固定）\n",
    "feature_cols = X.columns.tolist()\n",
    "X_test = test[feature_cols]\n",
    "\n",
    "# ✅ それぞれモデルで予測\n",
    "\n",
    "# CatBoost\n",
    "cat_preds = cat_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# XGBoost\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "xgb_preds = xgb_model.predict(dtest)\n",
    "\n",
    "# LightGBM\n",
    "lgb_preds = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ✅ Weighted Voting（等重み、必要に応じて調整可能）\n",
    "weights = [1, 1, 1]\n",
    "ensemble_preds = (weights[0] * cat_preds + weights[1] * xgb_preds + weights[2] * lgb_preds) / sum(weights)\n",
    "\n",
    "# ✅ Stackingも作成可能（必要に応じて切替）\n",
    "# stack_X_test = np.vstack([cat_preds, xgb_preds, lgb_preds]).T\n",
    "# ensemble_preds = meta_model.predict_proba(stack_X_test)[:, 1]\n",
    "\n",
    "# ✅ 提出用DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": original_test[\"Id\"],\n",
    "    \"Drafted\": ensemble_preds\n",
    "})\n",
    "\n",
    "# ✅ 保存ディレクトリをプロジェクトルートに作成\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "save_dir = os.path.join(root_dir, \"submissions\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ✅ Notebook名取得 → ファイル名決定\n",
    "try:\n",
    "    import ipynbname\n",
    "    notebook_path = ipynbname.path()\n",
    "    notebook_name = notebook_path.stem\n",
    "except:\n",
    "    notebook_name = \"20_0705_notebook\"\n",
    "\n",
    "match = re.search(r\"\\d{2}_\\d{4}\", notebook_name)\n",
    "tag = match.group() if match else notebook_name\n",
    "\n",
    "filename = f\"submission_{tag}().csv\"\n",
    "save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "# ✅ 書き出し\n",
    "submission.to_csv(save_path, index=False)\n",
    "print(f\"✅ アンサンブル提出ファイルを保存しました: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3e92759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 高相関ペア (|r| > 0.9):\n",
      "Broad_Jump & BurstScore: 0.9815\n",
      "Agility_3cone & AgilityScore: 0.9775\n",
      "Age_filled & Age_Speed: 0.9709\n",
      "Weight & Sprint_ASI: 0.9630\n",
      "Shuttle & AgilityScore: 0.9458\n",
      "Weight & BMI: 0.9386\n",
      "Position_encoded & Position_group_encoded: 0.9115\n",
      "Vertical_Jump & BurstScore: 0.9050\n",
      "BMI & Sprint_ASI: 0.9013\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 相関係数計算\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# ✅ 高相関ペア抽出\n",
    "threshold = 0.90\n",
    "high_corr = np.where(corr_matrix > threshold)\n",
    "high_corr_pairs = []\n",
    "\n",
    "for x, y in zip(*high_corr):\n",
    "    if x < y:\n",
    "        high_corr_pairs.append((\n",
    "            X.columns[x],\n",
    "            X.columns[y],\n",
    "            corr_matrix.iloc[x, y]\n",
    "        ))\n",
    "\n",
    "# ✅ 結果表示\n",
    "if high_corr_pairs:\n",
    "    print(f\"✅ 高相関ペア (|r| > {threshold}):\")\n",
    "    for col1, col2, corr in sorted(high_corr_pairs, key=lambda x: -x[2]):\n",
    "        print(f\"{col1} & {col2}: {corr:.4f}\")\n",
    "else:\n",
    "    print(f\"✅ 高相関ペアは存在しません (|r| > {threshold})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
